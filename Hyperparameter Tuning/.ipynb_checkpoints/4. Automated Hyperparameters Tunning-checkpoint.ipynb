{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c313ca",
   "metadata": {},
   "source": [
    "# Auto-Hyperparameters Tuning\n",
    "\n",
    "Automated Hyperparameter Tuning can be done by using techniques such as \n",
    "- 1. Bayesian Optimization\n",
    "- 2. Gradient Descent\n",
    "- 3. Evolutionary Algorithms\n",
    "\n",
    "## Bayesian Optimisation\n",
    "\n",
    "#### 1. Bayesian Optimization\n",
    "\n",
    "- The problem with grid and random search is that these are uninformed methods because they do not use the past results, Bayesian methods differ from random or grid search in that they use past evaluation results to choose the next values to evaluate. The concept is: limit expensive evaluations of the objective function by choosing the next input values based on those that have done well in the past.\n",
    "\n",
    "- Bayesian optimization uses probability to find the minimum of a function. The final aim is to find the input value to a function which can gives us the lowest possible output value.It usually performs better than random,grid and manual search providing better performance in the testing phase and reduced optimization time.\n",
    "\n",
    "### Four Part of Bayesian Optimization¶\n",
    "Bayesian hyperparameter optimization requires the same four parts as we implemented in grid and random search:\n",
    "\n",
    "1. **Objective Function**: takes in an input (hyperparameters) and returns a score to minimize or maximize (the cross validation score)\n",
    "2. **Domain space**: the range of input values (hyperparameters) to evaluate\n",
    "\n",
    "3. **Optimization Algorithm** : the method used to construct the surrogate function and choose the next values to evaluate\n",
    "\n",
    "4. **Results**: score, value pairs that the algorithm uses to build the surrogate function The only differences are that now our objective function will return a score to minimize (this is just convention in the field of optimization), our domain space will be probability distributions rather than a hyperparameter grid, and the optimization algorithm will be an informed method that uses past results to choose the next hyperparameter values to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d78cc8",
   "metadata": {},
   "source": [
    "# 1. Importing necessary libraries and data, Preprocessing and Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb37d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Loading Data\n",
    "df = pd.read_csv('C:/Users/Shubham/Documents/Data Science/Notebooks/00. Data_Store/preprocessed_diabetes.csv')\n",
    "\n",
    "# Splitting into Features and Target\n",
    "x = df.drop([\"Outcome\"], axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# Splitting into Train Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=33)\n",
    "\n",
    "# Defining the RandomForest Classifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c2e81",
   "metadata": {},
   "source": [
    "# 2. Using HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766026ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473e355",
   "metadata": {},
   "source": [
    "# 2.1 Domain Space - \n",
    "Creating the dictionary of hyperparameters \n",
    "\n",
    "The following will be used in this post:\n",
    "1. hp.choice(label, options) where options should be a python list or tuple.\n",
    "\n",
    "2. hp.normal(label, mu, sigma) where mu and sigma are the mean and standard deviation, respectively.\n",
    "\n",
    "3. hp.uniform(label, low, high) where low and high are the lower and upper bounds on the range.\n",
    "\n",
    "4. Others are available, such as hp.normal, hp.lognormal, hp.quniform, but we will not use them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10388cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': <hyperopt.pyll.base.Apply at 0x19e07384250>,\n",
       " 'max_depth': <hyperopt.pyll.base.Apply at 0x19e073843a0>,\n",
       " 'max_features': <hyperopt.pyll.base.Apply at 0x19e07384070>,\n",
       " 'min_samples_leaf': <hyperopt.pyll.base.Apply at 0x19e0732dca0>,\n",
       " 'min_samples_split': <hyperopt.pyll.base.Apply at 0x19e0732da00>,\n",
       " 'n_estimators': <hyperopt.pyll.base.Apply at 0x19e0732d8e0>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.quniform('max_depth', 10, 12, 10),\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.choice('n_estimators', [10, 50])\n",
    "        }\n",
    "space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc7165e",
   "metadata": {},
   "source": [
    "# 2.2. Objective Function -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "088896ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space[\"criterion\"],\n",
    "                                   max_depth = space[\"max_depth\"],\n",
    "                                   max_features = space[\"max_features\"],\n",
    "                                   min_samples_leaf = space[\"min_samples_leaf\"],\n",
    "                                   min_samples_split = space[\"min_samples_split\"],\n",
    "                                   n_estimators = space[\"n_estimators\"]\n",
    "                                  )\n",
    "    accuracy = cross_val_score(model, x_train, y_train, cv=2).mean()\n",
    "    \n",
    "    # As we minimise the function and want higher accuracy we return negative accuracy\n",
    "    \n",
    "    return {'loss': - accuracy, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4129d75",
   "metadata": {},
   "source": [
    "# 2.3. Optimization Algorithm -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55dbe99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 20/20 [00:01<00:00, 12.37trial/s, best loss: -0.752442996742671]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "           space = space,\n",
    "           algo = tpe.suggest,\n",
    "           max_evals = 20,\n",
    "           trials = trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a59cf1",
   "metadata": {},
   "source": [
    "# 2.4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "785eaec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 1,\n",
       " 'max_depth': 10.0,\n",
       " 'max_features': 0,\n",
       " 'min_samples_leaf': 0.023777739592153457,\n",
       " 'min_samples_split': 0.46628285738423525,\n",
       " 'n_estimators': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f78b8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = {0: 'entropy', 1: 'gini'}\n",
    "feat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\n",
    "est = {0: 10, 1: 50, 2: 75, 3: 100, 4: 125}\n",
    "\n",
    "rf = RandomForestClassifier(criterion = crit[best[\"criterion\"]],\n",
    "                           max_depth = best[\"max_depth\"],\n",
    "                           max_features = feat[best[\"max_features\"]],\n",
    "                           min_samples_leaf = best[\"min_samples_leaf\"],\n",
    "                           min_samples_split = best[\"min_samples_split\"],\n",
    "                           n_estimators = est[best[\"n_estimators\"]]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bee39a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10.0, min_samples_leaf=0.023777739592153457,\n",
       "                       min_samples_split=0.46628285738423525, n_estimators=10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9282e53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
